---
title: "CHU_H Final Project Step Four Modeling And Evaluation"
output: html_notebook
---

These are the libraries we are going to use for this step

```{r}
library(caTools)
library(FNN)
library(caret)
```


We first load the data we obtained from data preparation

```{r}
well.shaped.dataframe <- read.csv("AprilBeatmapDataWellShaped.csv")
```

Before we move on to training a model, we should take out the outliers we noticed during our data exploration phase.

```{r}
outlier.dataframe <- read.csv("outliers.csv")
outlier.beatmap_id <- outlier.dataframe$beatmap_id
well.shaped.dataframe.no.outlier <- subset(well.shaped.dataframe, !(beatmap_id %in% outlier.beatmap_id))
well.shaped.dataframe.no.outlier
```

We shoud also make a subset of the data to be our validation set. We will use 10% of our data to be our validation set since our data is relatively small

```{r}
set.seed(04172018)
sample <- sample.split(well.shaped.dataframe.no.outlier, SplitRatio = .9)
well.shaped.dataframe.no.outlier.train <- subset(well.shaped.dataframe.no.outlier, sample == TRUE)
well.shaped.dataframe.no.outlier.validation <- subset(well.shaped.dataframe.no.outlier, sample == FALSE)
well.shaped.dataframe.no.outlier.train
well.shaped.dataframe.no.outlier.validation
```

Since we learned from data exploration that our data is skewed, is better to use a model that does not assume normal distribution such as kNN regression. We will start with picking a k that is the square root of the number of dimension (not considering beatmapset_id, beatmap_id, and play_count_per_day itself)

```{r}
# Square root of 14 rounds to 4
# k = 4
id.column.names <- c("beatmapset_id","beatmap_id")
training.no.id <- well.shaped.dataframe.no.outlier.train[,!(names(well.shaped.dataframe.no.outlier.train) %in% id.column.names)]
validation.no.id <- well.shaped.dataframe.no.outlier.validation[,!(names(well.shaped.dataframe.no.outlier.validation) %in% id.column.names)]
pred.with.k.4 <- knn.reg(training.no.id,validation.no.id, y = playcount_per_day, k = 4)
```

TO verify accuracy we can plot the results sorted by play count per day

```{r}
plot(validation.no.id$playcount_per_day, col = "blue", ylab = "Play Count Per Day", main = "k = 4")
points(pred.with.k.4$pred,col = "orange")
lines(pred.with.k.4$pred, col = "red")
legend(x = "topleft",legend = c("Blue Points: Actual","Orange Points: Prediction"))
```

From the graph we can see that with knn perform rather well when playcount per day is low but perform rather poorly when the numbers are high. To evaluate the fitness of our data, we can find Mean Absolute Error (MAE) and the Mean Squared Error (MSE) when k = 4

```{r}
find.mae <- function(vec1,vec2) {
  return (sum(abs(vec1 - vec2)) / length(vec1))
}

find.mse <- function(vec1,vec2) {
  return (sum((vec1 - vec2)^2) / length(vec1))
}

pred.with.k.4 <- knn.reg(training.no.id,validation.no.id, y = playcount_per_day, k = 4)
pred.with.k.4.mae <- find.mae(pred.with.k.4$pred,validation.no.id$playcount_per_day)
pred.with.k.4.mse <- find.mse(pred.with.k.4$pred,validation.no.id$playcount_per_day)
pred.with.k.4.mae
pred.with.k.4.mse
```

To see if we can improve by choosing a better k, we can try more k from 1 to 20

```{r}
for (i in 1:20) {
  pred.with.different.k <- knn.reg(training.no.id,validation.no.id,y=playcount_per_day, k = i)
  plot(validation.no.id$playcount_per_day, col = "blue", ylab = "Play Count Per Day", main = paste("k = ",i))
  points(pred.with.different.k$pred, col = "orange")
  lines(pred.with.different.k$pred, col = "red")
  legend(x="topleft",legend = c("Blue Points: Actual","Orange Points: Prediction"))
}
```

We notice a trend that when k > 5 there is not much difference except the line gets tighter and tighter. We and take a closer look at how different k performs by looking at their mean absolute error and mean squared error

```{r}
k.values <- seq(1,5)
pred.with.k.1 <- knn.reg(training.no.id,validation.no.id, y = playcount_per_day, k = 1)
pred.with.k.1.mae <- find.mae(pred.with.k.1$pred,validation.no.id$playcount_per_day)
pred.with.k.1.mse <- find.mse(pred.with.k.1$pred,validation.no.id$playcount_per_day)

pred.with.k.2 <- knn.reg(training.no.id,validation.no.id, y = playcount_per_day, k = 2)
pred.with.k.2.mae <- find.mae(pred.with.k.2$pred,validation.no.id$playcount_per_day)
pred.with.k.2.mse <- find.mse(pred.with.k.2$pred,validation.no.id$playcount_per_day)

pred.with.k.3 <- knn.reg(training.no.id,validation.no.id, y = playcount_per_day, k = 3)
pred.with.k.3.mae <- find.mae(pred.with.k.3$pred,validation.no.id$playcount_per_day)
pred.with.k.3.mse <- find.mse(pred.with.k.3$pred,validation.no.id$playcount_per_day)

pred.with.k.5 <- knn.reg(training.no.id,validation.no.id, y = playcount_per_day, k = 5)
pred.with.k.5.mae <- find.mae(pred.with.k.5$pred,validation.no.id$playcount_per_day)
pred.with.k.5.mse <- find.mse(pred.with.k.5$pred,validation.no.id$playcount_per_day)

k.comparison.dataframe <- data.frame(k.values,c(pred.with.k.1.mae,pred.with.k.2.mae,pred.with.k.3.mae,pred.with.k.4.mae,pred.with.k.5.mae),
                                     c(pred.with.k.1.mse,pred.with.k.2.mse,pred.with.k.3.mse,pred.with.k.4.mse,pred.with.k.5.mse))
names(k.comparison.dataframe) <- c("k value","Mean Absolute Error","Mean Squared Error")
k.comparison.dataframe
```

Even though k = 5 has a slightly beeter MAE than k = 3, k = 3 has a much better MSE, partly because by having a smaller k allows for a higher variance which allows it to do a better job at predicting data with play count per day that are higher than usual. Therefore we think k = 3 is the best k

Let's take a look at a different model, a multiple regression model.

As we discovered in data exploration, our data is positive skewed which means we should probably apply some transformation that can make play count per day sufficiently normalized. Here we experiment different types of transformation and try to find which performs the best

```{r}
natural.log.transformed.playcount.per.day <- log(well.shaped.dataframe.no.outlier$playcount_per_day)
log10.transformed.playcount.per.day <- log10(well.shaped.dataframe.no.outlier$playcount_per_day)
square.root.transformed.playcount.per.day <- (well.shaped.dataframe.no.outlier$playcount_per_day)^(1/2)
cube.root.transformed.playcount.per.day <- (well.shaped.dataframe.no.outlier$playcount_per_day)^(1/3)

par(mfrow=c(3,2))
hist(well.shaped.dataframe.no.outlier$playcount_per_day, xlab = "Play Count Per Day", main = "Original Distribution")
hist(natural.log.transformed.playcount.per.day, xlab = "Natural Log Play Count Per Day", main = "Natural Log Transformed Distribution")
hist(log10.transformed.playcount.per.day, xlab = "log10 Play Count Per Day", main = "Log 10 Transformed Distribution")
hist(square.root.transformed.playcount.per.day,  xlab = "Square Root Play Count Per Day", main = "Square Root Transformed Distribution")
hist(cube.root.transformed.playcount.per.day, xlab = "Cube Root Play Count Per Day", main = "Cube Root Transformed Distribution")
```

Judging from the graph, the cube root transformation is clearly the one that best normalize our data, therefore we are going to cube root transform our play count per day in our training and validation set

```{r}
cube.root.transformed.training <- training.no.id
cube.root.transformed.training$playcount_per_day <- (training.no.id$playcount_per_day) ^ (1/3)
cube.root.transformed.training
cube.root.transformed.validation <- validation.no.id
cube.root.transformed.validation$playcount_per_day <- (validation.no.id$playcount_per_day) ^ (1/3)
cube.root.transformed.validation
```

Now we can build our multiple linear regression model

```{r}
multiple.linear.regression.model.initial <- lm(formula = playcount_per_day ~ .,data = cube.root.transformed.training)
summary(multiple.linear.regression.model.initial)
```

To improve our model we can use backward fitting. This means the first variable we should remove is diff_overall

```{r}
multiple.linear.regression.model.second <- lm(formula = playcount_per_day ~ . - diff_overall,data = cube.root.transformed.training)
summary(multiple.linear.regression.model.second)
```

The next feature with highest p-value is diff_drain

```{r}
multiple.linear.regression.model.third <- lm(formula = playcount_per_day ~ . - diff_overall -diff_drain,data = cube.root.transformed.training)
summary(multiple.linear.regression.model.third)
```

The features with the highest p-value right now are genre_anime and genre_pop but since they are categorical variable we choose not to remove it and move on to the next highest feature with the higher p-value: difficultyrating

```{r}
multiple.linear.regression.model.fourth <- lm(formula = playcount_per_day ~ . - diff_overall -diff_drain -difficultyrating,data = cube.root.transformed.training)
summary(multiple.linear.regression.model.fourth)
```

the next highest p-value non-categorical feature is max_combo

```{r}
multiple.linear.regression.model.fifth <- lm(formula = playcount_per_day ~ . - diff_overall -diff_drain -difficultyrating -diff_size -max_combo,data = cube.root.transformed.training)
summary(multiple.linear.regression.model.fifth)
```

Now that our model has R-square > 0.70 and all non-categorical features have p value < 0.05, we can start using this model and see how well it works against the validation set. Since we cubed root transformed our predicting variable, we will backtransform by raising the result to the power of 3

```{r}
multiple.linear.regression.model.prediction <- predict(multiple.linear.regression.model.fifth,cube.root.transformed.validation)
multiple.linear.regression.model.prediction.backtransformed <-(multiple.linear.regression.model.prediction) ^ 3
multiple.linear.regression.model.prediction.backtransformed
plot(validation.no.id$playcount_per_day, ylab = "playcount_per_day", main = "Multiple Linear Regression Model", col = "blue")
points(multiple.linear.regression.model.prediction.backtransformed, col="orange")
lines(multiple.linear.regression.model.prediction.backtransformed, col="red")
legend(x="topleft",legend = c("Blue Points: Actual","Orange Points: Prediction"))
```

To evaluate how good our model is we use a similar metric to the ones we used to evaluate kNN regression model (MAE and MSE)

```{r}
multiple.linear.regression.model.mae <- find.mae(multiple.linear.regression.model.prediction.backtransformed,validation.no.id$playcount_per_day)
multiple.linear.regression.model.mse <- find.mse(multiple.linear.regression.model.prediction.backtransformed,validation.no.id$playcount_per_day)
multiple.linear.regression.model.mae
multiple.linear.regression.model.mse
```

Comparing between the kNN regression model and the multiple linear regression model, we think the kNN regression model is the superior one. This is because the multiple linear regression model have a much higher MAE and MSE. Not only so, multiple linear regression suffers when the dataset is small which is true in our case. Furthermore, kNN makes no assumption of the distribution of the data, so no transformations are needed and accuracy can be improved.

However, this does not mean the multiple linear regression model is not useful, in fact, we can see what propoerties would make a beatmap popular thanks to the coefficients that multiple linear regression model brought up:

```{r}
multiple.linear.regression.model.fifth$coefficients
```

If we order by coefficients, we can see that the most popular properties of a beatmap are:

approved_ranked -> language_japanese -> diff_approach -> favourite_count

and the most negative influential properties of a beatmaps ordered from least influence to most influence are:

bpm -> hit_length -> genre_anime -> genre_pop

Intepretation of the positive properties:

In osu! there are many approval status, being simply approved, qualified, or ranked. To rank a beatmap it has to go through a lot of inspection and re-editing, which means ranked beatmaps usually have higher quality than the others. The osu! website also has its default beatmap page filtered by beatmaps that are ranked, which might also contribute to ranked beatmaps having high play count per day.

Traditionally, there have been a lot of overlap between the osu community and the anime community, and since most anime music are in japanese, it is natural that beatmap made from japanese song are more popular than the others.

diff_approach refers to a mechanic in the game that is called "approach rate", which indicates how fast the ring appraoches the circle. From this we can see that players seem to enjoy beatmaps with higher approach rate than lower.

favorite_count refers to the number of players that marked the beatmap as favorite, it seems like the more player that mark a beatmap as a favorite, the more likely that some other player would like to play.

Intepretation of the negative properties:

bpm stands for beats per minute, a high bpm means the song is fast and vice versa. From this we can see that people seem to enjoy beatmaps that are slower rather than faster.

hit_length stands for the time between the first hit object appears and the last hit object appears, it can be roughly translated to the length of the song. From this we find that player tend to enjoy beatmaps that are shorter than the others.

genre_anime and genre_pop:
Although from the coefficient it seem like players really do not enjoy anime or pop song, the p value for these two faetures are around 0.3 which is way above the convention 0.05. Therefore these two features shouldn't be considered as factors that would affect the popularity of a beatmap.

In conclusion,

The "ideal beatmap" to a player, or a beatmap that would gain the most play count per day, would have the following property:
-- is ranked
-- is in japanese
-- has a high approach rate
-- has been liked by other players
-- is a slower song
-- is short

In fact, the most played beatmap in osu so far, aside from the default song, is a beatmap based on a song named "No Title" by Reol, and it matches 5 out of the 6 properties.

-- is ranked (check)
-- is in japanese (check)
-- has a high approach rate (9.6 / 10, check)
-- has been liked by other players (check)
-- is a slower song (bpm is 200 which is higher than the average of 167, Not check)
-- is short (only 70 seconds, below average 141 seconds, check)
